.. _speedup:

3 Min Quick Example: Docker + Speedup with vLLM
===============================================

Introduction
-------------------------

.. figure:: figures/demo_1.png
    :width: 70%
    :align: center

This demo shows that different vLLM instances can share the prefix KV cache between each other by using LMCache on a single node, so that the KV cache generated by one vLLM instance can be reused by another.

.. note::
    Though this demo focuses on single-node case, it can be generalized to allow KV cache sharing between any two vLLM instances in the cluster, as long as they have a commonly-shared NFS disk.

Prerequisites
-------------------------

* 4 Nvidia A6000 or A40 GPU on the same node
* Local SSD disk with peak IO bandwidth > 3GB/s
* `docker compose <https://docs.docker.com/compose/install/>`_ installed on the machine
* sudo access to run ``docker compose up``
* A huggingface token with access to ``mistralai/Mistral-7B-Instruct-v0.2``. 

.. note::
    For more information on Huggingface login, please refer 
    to the `Huggingface documentation <https://huggingface.co/docs/huggingface_hub/en/quick-start>`_.

Customise the configuration
----------------------------

Note that this demo runs the model ``mistralai/Mistral-7B-Instruct-v0.2`` and stores KV cache under directory ``/tmp``.
You can customize these two entires by editing the file ``.env`` under ``demo4-compare-with-vllm``.

Run the demo
-------------------------

.. code-block:: bash

    git clone https://github.com/LMCache/demo.git
    cd demo/demo4-compare-with-vllm
    echo "HF_TOKEN=<your HF token>" >> .env
    sudo docker compose up -d
    timeout 300 bash -c '
        until curl -X POST localhost:8000/v1/completions > /dev/null 2>&1; do
        echo "waiting for server to start..."
        sleep 1
        done' # wait for the docker compose to be ready for receiving requests
    streamlit run frontend.py

Please replace ``<your HF token>`` with your huggingface token in the bash script above.

You can see the following output in the terminal when the application is ready:

.. code-block:: text

    You can now view your Streamlit app in your browser.

    Local URL: http://localhost:8501
    Network URL: http://<Your Public IP>:8501
    External URL: http://<Your Public IP>:8501

Send your requests to different serving engines
------------------------------------------------

After opening the above link in your browser, you will see a webpage with the following layout
(may need to ``preheat`` for ~30 seconds).

.. figure:: figures/demo_2.png
    :width: 90%
    :align: center

* You can select different serving engines in the ``orange area``

    * Two engines are running vLLM (0.6.2) + LMCache
    * Two engines are running official vLLM (0.6.2)

* You can ask the LLM questions in the ``green area``.
* The right-hand side ``(blue area)`` shows the "context" of the query sent to the LLM.

You can try sending different requests to different serving engines.

Expected results
-------------------------

The first query
^^^^^^^^^^^^^^^^

When the first request is sent to the original ``vLLM (A)`` or ``vLLM w/ LMCache (A)``, both engines will have a similar response delay.

.. figure:: figures/demo_3.png
    :width: 97%
    :align: center

Querying different engines with the same context
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

However, when the request with the same context is sent to the ``vLLM w/ LMCache (B)``, the engine can load the KV cache shared from ``vLLM w/ LMCache (A)``, 
and thus have a much better response delay.

.. figure:: figures/demo_4.png
    :width: 90%
    :align: center

Clean up
-------------------------

Use ``Ctrl+C`` to terminate the frontend, and then run ``sudo docker compose down`` 
to shut down the service.

Known issues
-------------------------

Currently, LMCache may impact the decoding speed of the vLLM when storing the KV cache to the disk. 
This is a known issue, and can be solved by better async implementation soon.

.. note::
   Source code for this demo is available at `demo <https://github.com/LMCache/demo/tree/master/demo4-compare-with-vllm>`_.



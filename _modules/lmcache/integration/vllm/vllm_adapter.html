<!DOCTYPE html>
<html lang="en"
      data-content_root="../../../../"
      x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }"
      x-init="$watch('darkMode', val => localStorage.setItem('darkMode', val))"
      class="scroll-smooth"
      :class="{'dark': darkMode === 'dark' || (darkMode === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)}"
>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta charset="utf-8" />
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="white" />
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="black" />
  
    <title>lmcache.integration.vllm.vllm_adapter | LMCache</title>
    <meta property="og:title" content="lmcache.integration.vllm.vllm_adapter | LMCache" />
    <meta name="twitter:title" content="lmcache.integration.vllm.vllm_adapter | LMCache" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme.css?v=caad1007" />
      <link rel="icon" href="../../../../_static/lmcache-logo.png" />
        <link rel="search" title="Search" href="../../../../search.html" />
        <link rel="index" title="Index" href="../../../../genindex.html" />

    <script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
</head>
<body x-data="{ showSidebar: false }" class="min-h-screen font-sans antialiased bg-background text-foreground" :class="{ 'overflow-hidden': showSidebar }">
    <div x-cloak x-show="showSidebar" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" @click.self="showSidebar = false"></div><div id="page" class="relative flex flex-col min-h-screen"><a href="#content" class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100">
      Skip to content
    </a><header
  class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
    <div class="hidden mr-4 md:flex">
      <a href="../../../../index.html" class="flex items-center mr-6"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">LMCache</span>
      </a></div><button
      class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden"
      type="button" @click="showSidebar = true">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" aria-hidden="true"
        fill="currentColor">
        <path
          d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z" />
      </svg>
      <span class="sr-only">Toggle navigation menu</span>
    </button>
    <div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
      <div class="flex-1 w-full md:w-auto md:flex-none"><form id="searchbox"
      action="../../../../search.html"
      method="get"
      class="relative flex items-center group"
      @keydown.k.window.meta="$refs.search.focus()">
  <input x-ref="search"
          name="q"
          id="search-input"
          type="search"
          aria-label="Search the docs"
          placeholder="Search ..."
          class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" />
  <kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
    <span class="text-xs">âŒ˜</span>
    K
  </kbd>
</form>
      </div>
      <nav class="flex items-center space-x-1">
        <a href="https://github.com/LMCache/LMCache/" title="Visit GitHub" rel="noopener nofollow">
          <div
            class="inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md disabled:opacity-50 disabled:pointer-events-none hover:bg-accent hover:text-accent-foreground h-9 w-9">
            <svg height="26px" style="margin-top:-2px;display:inline" viewBox="0 0 45 44" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M22.477.927C10.485.927.76 10.65.76 22.647c0 9.596 6.223                 17.736 14.853 20.608 1.087.2 1.483-.47 1.483-1.047 0-.516-.019-1.881-.03-3.693-6.04 1.312-7.315-2.912-7.315-2.912-.988-2.51-2.412-3.178-2.412                 -3.178-1.972-1.346.149-1.32.149-1.32 2.18.154 3.327 2.24 3.327 2.24 1.937 3.318 5.084 2.36 6.321 1.803.197-1.403.759-2.36 1.379-2.903-4.823-.548-9.894-2.412-9.894-10.734 0-2.37.847-4.31 2.236-5.828-.224-.55-.969-2.759.214-5.748 0 0 1.822-.584 5.972 2.226 1.732-.482 3.59-.722 5.437-.732 1.845.01 3.703.25 5.437.732 4.147-2.81 5.967-2.226 5.967-2.226 1.185 2.99.44 5.198.217 5.748 1.392 1.517 2.232                  3.457 2.232 5.828 0 8.344-5.078 10.18-9.916 10.717.779.67 1.474 1.996 1.474                 4.021 0 2.904-.027 5.247-.027 5.96 0 .58.392 1.256 1.493 1.044C37.981 40.375 44.2 32.24                  44.2 22.647c0-11.996-9.726-21.72-21.722-21.72" fill="currentColor"/></svg>
          </div>
        </a>
        
        <button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'"
          class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9"
          type="button"
          aria-label="Color theme switcher">
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0">
            <path
              d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z" />
          </svg>
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100">
            <path
              d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z" />
          </svg>
        </button>
      </nav>
    </div>
  </div>
</header>

    <div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside id="left-sidebar"
  class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky"
  :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }">

    <a href="../../../../index.html" class="!justify-start text-sm md:!hidden bg-background"><span class="font-bold text-clip whitespace-nowrap">LMCache</span>
    </a>

    <div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
      <div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
  <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/speedup.html">3 Min Quick Example: Docker + Speedup with vLLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/installation.html">Pip Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started/docker.html">Docker Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Configure LMCache</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../configuration/v1/index.html">LMCache v1</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../configuration/v1/v1_config.html">Configuring LMCache v1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../configuration/v0/index.html">LMCache v0</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../configuration/v0/v0_config.html">Configuring LMCache v0</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Detailed Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/v1/index.html">LMCache v1</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v1/cpu_offload.html">CPU Offloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v1/disagg.html">Disaggregated prefill</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v1/p2p.html">P2P KV Cache Sharing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/v0/index.html">LMCache v0</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v0/backend.html">Selecting a backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v0/launching.html">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v0/save_decode.html">Saving the decode cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v0/kv_blending.html">KV blending</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/v0/measuring_improvements.html">End-to-end measurements</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../models/models.html">Supported Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer_tutorial/overview.html">LMCache Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer_tutorial/LLM_Engine.html">LLM Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer_tutorial/LMCache_Engine.html">LMCache Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer_tutorial/LMCache_Backend.html">LMCache Backend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../advanced/lmcache.blend.html">LMCache Blend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../advanced/lmcache.server.html">LMCache Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../advanced/lmcache.storage_backend.html">LMCache Storage Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../advanced/lmcache.experimental.html">LMCache Experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../advanced/lmcache.integration.html">LMCache Integration</a></li>
</ul>

</nav>
      </div>
    </div>
    <button type="button" @click="showSidebar = false"
      class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
        stroke="none" class="h-4 w-4">
        <path
          d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z" />
      </svg>
    </button>
  </aside>
        <main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs"
     class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
  <a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground"
     href="../../../../index.html">
    <span class="hidden md:inline">LMCache</span>
    <svg xmlns="http://www.w3.org/2000/svg"
         height="18"
         width="18"
         viewBox="0 96 960 960"
         aria-label="Home"
         fill="currentColor"
         stroke="none"
         class="md:hidden">
      <path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z" />
    </svg>
  </a>
  
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../../../index.html">Module code</a>
    
<div class="mr-1">/</div><span aria-current="page"
        class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">lmcache.integration.vllm.vllm_adapter</span>
</nav>

    <div id="content" role="main">
      <h1>Source code for lmcache.integration.vllm.vllm_adapter</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">dataclasses</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">enum</span><span class="w"> </span><span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">types</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleNamespace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils.rnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">pad_sequence</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.worker.model_runner</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelInputForGPUWithSamplingMetadata</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.attention.backends.flash_attn</span><span class="w"> </span><span class="kn">import</span> <span class="n">FlashAttentionMetadata</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">CacheConfig</span><span class="p">,</span> <span class="n">ModelConfig</span><span class="p">,</span> <span class="n">ParallelConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.sequence</span><span class="w"> </span><span class="kn">import</span> <span class="n">SequenceGroupMetadata</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_kv_cache_torch_dtype</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">LMCacheEngineMetadata</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.experimental.cache_engine</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span><span class="n">LMCacheEngine</span><span class="p">,</span>
                                               <span class="n">LMCacheEngineBuilder</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.experimental.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">LMCacheEngineConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.experimental.gpu_connector</span><span class="w"> </span><span class="kn">import</span> <span class="n">VLLMPagedMemGPUConnectorV2</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.integration.vllm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">ENGINE_NAME</span><span class="p">,</span> <span class="n">lmcache_get_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.logging</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_logger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_lmcache_nvtx_annotate</span>

<span class="c1"># FIXME(Jiayi): temporarily comment this out</span>
<span class="c1">#from lmcache_vllm.blend_adapter import remove_request_id_indices</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">init_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">LMCACHE_CUDA_STREAM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>


<div class="viewcode-block" id="StoreStatus">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.StoreStatus">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">StoreStatus</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PREFILL</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">CHUNK_PREFILL</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">DECODE</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">SUFFIX_PREFILL</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">5</span></div>



<div class="viewcode-block" id="RetrieveStatus">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.RetrieveStatus">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">RetrieveStatus</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">PREFILL</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">CHUNK_PREFILL</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># not last chunk</span>
    <span class="n">CHUNK_PREFILL_LAST</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">4</span></div>



<span class="n">SUPPORTED_MODELS</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">(</span>
    <span class="n">llama_family</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span><span class="p">],</span>
    <span class="n">longchat_family</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lmsys/longchat-7b-16k&quot;</span><span class="p">],</span>
    <span class="n">mistral_family</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.2&quot;</span><span class="p">],</span>
    <span class="n">glm_family</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;THUDM/glm-4-9b-chat&quot;</span><span class="p">],</span>
    <span class="n">qwen_family</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Qwen/Qwen-7B&quot;</span><span class="p">],</span>
<span class="p">)</span>


<div class="viewcode-block" id="ModelInputSubset">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.ModelInputSubset">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ModelInputSubset</span><span class="p">:</span>
    <span class="n">model_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span>
    <span class="n">attn_layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span>
    <span class="n">start_layer</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end_layer</span><span class="p">:</span> <span class="nb">int</span></div>



<div class="viewcode-block" id="create_model_input_subset">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.create_model_input_subset">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">create_model_input_subset</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model_executable</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelInputSubset</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">SUPPORTED_MODELS</span><span class="o">.</span><span class="n">llama_family</span> <span class="ow">or</span> \
        <span class="n">model_name</span> <span class="ow">in</span> <span class="n">SUPPORTED_MODELS</span><span class="o">.</span><span class="n">mistral_family</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_executable</span><span class="o">.</span><span class="n">model</span>
        <span class="n">model_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">attn_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">self_attn</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_layers</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">model_name</span> <span class="ow">in</span> <span class="n">SUPPORTED_MODELS</span><span class="o">.</span><span class="n">glm_family</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_executable</span><span class="o">.</span><span class="n">transformer</span>
        <span class="n">model_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">attn_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">self_attention</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_layers</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># FIXME(Jiayi): `else` is the default setting, which could be wrong</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_executable</span><span class="o">.</span><span class="n">model</span>
        <span class="n">model_layers</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span>
        <span class="n">attn_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">self_attn</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_layers</span><span class="p">]</span>

    <span class="c1"># FIXME(Jiayi): ChatGLM does not have `model` or `start_layer`</span>
    <span class="c1"># How does PP work in this case?</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;start_layer&quot;</span><span class="p">):</span>
        <span class="n">start_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">start_layer</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_layer</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;end_layer&quot;</span><span class="p">):</span>
        <span class="n">end_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">end_layer</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">end_layer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_layers</span><span class="p">)</span>

    <span class="n">model_input_subset</span> <span class="o">=</span> <span class="n">ModelInputSubset</span><span class="p">(</span><span class="n">model_layers</span><span class="o">=</span><span class="n">model_layers</span><span class="p">,</span>
                                          <span class="n">attn_layers</span><span class="o">=</span><span class="n">attn_layers</span><span class="p">,</span>
                                          <span class="n">start_layer</span><span class="o">=</span><span class="n">start_layer</span><span class="p">,</span>
                                          <span class="n">end_layer</span><span class="o">=</span><span class="n">end_layer</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model_input_subset</span></div>



<span class="c1"># FIXME(Jiayi): temporarily comment this out</span>
<span class="c1">#def lmcache_remove_request_id_indices(request_id):</span>
<span class="c1">#    engine = LMCacheEngineBuilder.get(ENGINE_NAME)</span>
<span class="c1">#    if engine is None:</span>
<span class="c1">#        return</span>
<span class="c1">#    if not engine.config.enable_blending:</span>
<span class="c1">#        return</span>
<span class="c1">#    remove_request_id_indices(request_id)</span>


<div class="viewcode-block" id="init_lmcache_engine">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.init_lmcache_engine">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">init_lmcache_engine</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span>
    <span class="n">parallel_config</span><span class="p">:</span> <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="n">cache_config</span><span class="p">:</span> <span class="n">CacheConfig</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LMCacheEngine</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the LMCache engine by the given model config and parallel </span>
<span class="sd">    config. This function will check the environment variable </span>
<span class="sd">    `LMCACHE_CONFIG_FILE` to load the configuration file. If that environment</span>
<span class="sd">    variable is not set, this function will return None.</span>

<span class="sd">    :param model_config: The model configuration in vLLM.</span>
<span class="sd">    :type model_config: ModelConfig</span>
<span class="sd">    :param parallel_config: The parallel configuration in vLLM.</span>
<span class="sd">    :type parallel_config: ParallelConfig</span>
<span class="sd">    :param cache_config: The KV cache configuration in vLLM.</span>
<span class="sd">    :type cache_config: CacheConfig</span>

<span class="sd">    :return: The initialized LMCache engine or None (if the environment variable</span>
<span class="sd">        `LMCACHE_CONFIG_FILE` is not set).</span>
<span class="sd">    :rtype: Optional[LMCacheEngine]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">config</span> <span class="o">=</span> <span class="n">lmcache_get_config</span><span class="p">()</span>

    <span class="n">kv_dtype</span> <span class="o">=</span> <span class="n">get_kv_cache_torch_dtype</span><span class="p">(</span><span class="n">cache_config</span><span class="o">.</span><span class="n">cache_dtype</span><span class="p">,</span>
                                        <span class="n">model_config</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># construct kv shape (for mem pool)</span>
    <span class="n">num_layer</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get_num_layers</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span>
    <span class="n">num_kv_head</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get_num_kv_heads</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
    <span class="n">head_size</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">get_head_size</span><span class="p">()</span>
    <span class="n">kv_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layer</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">num_kv_head</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span>

    <span class="c1"># Change current device.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">LMCacheEngineMetadata</span><span class="p">(</span><span class="n">model_config</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                                     <span class="n">parallel_config</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
                                     <span class="n">parallel_config</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="s2">&quot;vllm&quot;</span><span class="p">,</span> <span class="n">kv_dtype</span><span class="p">,</span>
                                     <span class="n">kv_shape</span><span class="p">)</span>
    <span class="n">hidden_dim_size</span> <span class="o">=</span> <span class="n">num_kv_head</span> <span class="o">*</span> <span class="n">head_size</span>
    <span class="n">vllm_gpu_connector</span> <span class="o">=</span> <span class="n">VLLMPagedMemGPUConnectorV2</span><span class="p">(</span><span class="n">hidden_dim_size</span><span class="p">,</span> <span class="n">num_layer</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">LMCacheEngineConfig</span><span class="p">),</span> \
        <span class="s2">&quot;LMCache experimental configuration is should be passed.&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get_or_create</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">metadata</span><span class="p">,</span>
                                                <span class="n">vllm_gpu_connector</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">engine</span></div>



<span class="c1"># TODO(Jiayi): This function is not used for now</span>
<div class="viewcode-block" id="broadcast_seq_group_metadata">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.broadcast_seq_group_metadata">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">broadcast_seq_group_metadata</span><span class="p">(</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
    <span class="n">is_driver_worker</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcast the `model_input` from driver worker to non-driver workers.</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>

<span class="sd">    :param is_driver_worker: Whether the code is executed in driver worker. </span>
<span class="sd">    :type is_driver_worker: bool</span>

<span class="sd">    : return: Original `model_input` if driver_worker.</span>
<span class="sd">              Broadcasted `model_input` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># broadcast len of `seq_group_metadata_list`</span>
    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">seq_group_metadata_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">seq_group_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">seq_group_metadata_list</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">seq_group_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">seq_group_len_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">seq_group_len</span> <span class="o">=</span> <span class="n">seq_group_len_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># broadcast `seq_group_metadata_list`</span>
    <span class="n">seq_group_metadata_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">]]</span>
    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">seq_group_metadata_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">seq_group_metadata_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">seq_group_metadata_list</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">seq_group_metadata_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">seq_group_len</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">seq_group_metadata_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
            <span class="n">model_input</span><span class="p">,</span>
            <span class="n">seq_group_metadata_list</span><span class="o">=</span>\
                <span class="n">seq_group_metadata_list</span> <span class="c1"># type: ignore[arg-type]</span>

        <span class="p">)</span></div>



<span class="c1"># TODO(Jiayi): This function is not used for now</span>
<div class="viewcode-block" id="broadcast_seq_group_list">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.broadcast_seq_group_list">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">broadcast_seq_group_list</span><span class="p">(</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
    <span class="n">is_driver_worker</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcast the `model_input` from driver worker to non-driver workers.</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>

<span class="sd">    :param is_driver_worker: Whether the code is executed in driver worker. </span>
<span class="sd">    :type is_driver_worker: bool</span>

<span class="sd">    : return: Original `model_input` if driver_worker.</span>
<span class="sd">              Broadcasted `model_input` otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># broadcast len of `seq_group_metadata_list`</span>
    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">seq_group_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">seq_group_len_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">seq_group_len_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">seq_group_len</span> <span class="o">=</span> <span class="n">seq_group_len_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># broadcast `seq_groups`</span>
    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="n">seq_groups</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>  <span class="c1"># type: ignore</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">seq_groups</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">seq_group_len</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">seq_groups</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_driver_worker</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_input</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sampling_metadata</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span>
        <span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span> <span class="o">=</span> <span class="n">seq_groups</span>  <span class="c1"># type: ignore</span>
        <span class="k">return</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span>
                                   <span class="n">sampling_metadata</span><span class="o">=</span><span class="n">sampling_metadata</span><span class="p">)</span></div>



<div class="viewcode-block" id="close_lmcache_engine">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.close_lmcache_engine">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">close_lmcache_engine</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Close the LMCache engine if it is initialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Closing LMCache Engine&quot;</span><span class="p">)</span>
    <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">destroy</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span></div>



<span class="c1"># FIXME(Jiayi): Need to modify this for lmcache_connector</span>
<span class="c1"># This function is not used for now</span>
<div class="viewcode-block" id="lmcache_should_retrieve">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.lmcache_should_retrieve">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">lmcache_should_retrieve</span><span class="p">(</span>
        <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
        <span class="n">kv_caches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">RetrieveStatus</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check should we retrieve KV from LMCache for the current model_input.</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>

<span class="sd">    :param kv_caches: The paged memory</span>
<span class="sd">    :type kv_caches: List[torch.Tensor]</span>

<span class="sd">    :return: RetrieveStatus.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">FlashAttentionMetadata</span><span class="p">),</span> \
        <span class="s2">&quot;Only FlashAttention backend is supported for now.&quot;</span>

    <span class="c1"># model_input doesn&#39;t have seq_lens in tp</span>
    <span class="c1"># but attn_metadata does</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span>
    <span class="k">assert</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">has_engine</span> <span class="o">=</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_engine</span> <span class="ow">or</span> <span class="n">kv_caches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">NONE</span>

    <span class="n">attn_meta</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span>
    <span class="n">prefill_meta</span> <span class="o">=</span> <span class="n">attn_meta</span><span class="o">.</span><span class="n">prefill_metadata</span>

    <span class="c1"># check if the current run is profiling</span>
    <span class="n">is_profile_run</span> <span class="o">=</span> <span class="p">(</span><span class="n">kv_caches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">kv_caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_profile_run</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">NONE</span>

    <span class="c1"># check if the current run is prefill</span>
    <span class="c1"># TODO (Jiayi): chunked prefill + prefix caching in a single batch</span>
    <span class="c1"># is not and should not be supported here</span>
    <span class="c1"># what about multiple chunk prefills in a single batch??</span>

    <span class="c1"># Assume all chunks are prefills</span>
    <span class="n">is_all_prefill_run</span> <span class="o">=</span> <span class="p">((</span><span class="n">attn_meta</span><span class="o">.</span><span class="n">num_prefills</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">))</span>\
        <span class="ow">and</span> <span class="n">prefill_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_all_prefill_run</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">selected_token_indices</span> <span class="o">=</span> \
            <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">selected_token_indices</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_token_indices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># There should only be 1 chunk in chunked prefill</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="k">return</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span>

        <span class="c1"># `&lt;` means chunked prefill is batched with decode</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_token_indices</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">PREFILL</span>

    <span class="k">return</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">NONE</span></div>



<span class="c1"># FIXME(Jiayi): Need to modify this for lmcache_connector</span>
<span class="c1"># This function is not used for now</span>
<div class="viewcode-block" id="lmcache_should_store">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.lmcache_should_store">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">lmcache_should_store</span><span class="p">(</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">StoreStatus</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check should we store KV into LMCache for the current model_input.</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>


<span class="sd">    :return: A list of StoreStatus.</span>
<span class="sd">             StoreStatus.PREFILL/DECODE/CHUNK_PREFILL if </span>
<span class="sd">             we should store KV after PREFILL/DECODE.</span>
<span class="sd">             StoreStatus.NONE if no storing is required.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">is_blend_effective</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if the blend is effective for the current request</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">blend_metadata</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="s2">&quot;blend_metadata&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">blend_metadata</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">return</span> <span class="n">blend_metadata</span><span class="o">.</span><span class="n">processed_layer_count</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">FlashAttentionMetadata</span><span class="p">),</span> \
        <span class="s2">&quot;Only FlashAttention backend is supported for now.&quot;</span>

    <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span>
    <span class="k">assert</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">store_status</span> <span class="o">=</span> <span class="p">[</span><span class="n">StoreStatus</span><span class="o">.</span><span class="n">NONE</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">)</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span>
    <span class="n">has_engine</span> <span class="o">=</span> <span class="n">engine</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">has_engine</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">store_status</span>
    <span class="k">assert</span> <span class="n">engine</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">attn_meta</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span>
    <span class="n">prefill_meta</span> <span class="o">=</span> <span class="n">attn_meta</span><span class="o">.</span><span class="n">prefill_metadata</span>

    <span class="c1"># Don&#39;t store if this request is processed by cacheblend</span>
    <span class="k">if</span> <span class="n">is_blend_effective</span><span class="p">(</span><span class="n">attn_meta</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">store_status</span>

    <span class="n">is_all_prefill_run</span> <span class="o">=</span> <span class="p">((</span><span class="n">attn_meta</span><span class="o">.</span><span class="n">num_prefills</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">))</span>\
        <span class="ow">and</span> <span class="p">(</span><span class="n">prefill_meta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">is_all_prefill_run</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">broadcast_seq_group_list</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">seq_group_list</span>
                                               <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>
        <span class="k">assert</span> <span class="n">seq_group_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">selected_token_indices</span> <span class="o">=</span> \
            <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">selected_token_indices</span>

        <span class="n">seq_data_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">selected_token_indices_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">seq_group_idx</span><span class="p">,</span> <span class="n">seq_group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_group_list</span><span class="p">):</span>

            <span class="c1"># TODO(Jiayi): Figure out scenarios (other than chunk prefill)</span>
            <span class="c1"># where `do_sample`` is False</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">seq_group</span><span class="o">.</span><span class="n">do_sample</span><span class="p">:</span>
                <span class="n">store_status</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span>
                <span class="n">seq_data_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">continue</span>

            <span class="k">for</span> <span class="n">seqid</span><span class="p">,</span> <span class="n">seq_data</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="o">.</span><span class="n">seq_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">seq_data</span><span class="o">.</span><span class="n">get_len</span><span class="p">(</span>
                <span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">!=</span> <span class="n">selected_token_indices</span><span class="p">[</span><span class="n">selected_token_indices_idx</span><span class="p">]:</span>
                    <span class="c1"># last chunk in chunk prefill</span>
                    <span class="c1"># or prefix already hit in retrieve</span>
                    <span class="n">store_status</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">SUFFIX_PREFILL</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">store_status</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">PREFILL</span>
                <span class="n">seq_data_idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">selected_token_indices_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">store_status</span>

    <span class="c1"># Determine whether to save decoded KV cache</span>
    <span class="k">if</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">save_decode_cache</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_lens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">seq_len</span> <span class="o">%</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">store_status</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">DECODE</span>
    <span class="k">return</span> <span class="n">store_status</span></div>



<div class="viewcode-block" id="lmcache_store_kv">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.lmcache_store_kv">[docs]</a>
<span class="nd">@_lmcache_nvtx_annotate</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lmcache_store_kv</span><span class="p">(</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">ModelConfig</span><span class="p">,</span>
    <span class="n">parallel_config</span><span class="p">:</span> <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="n">cache_config</span><span class="p">:</span> <span class="n">CacheConfig</span><span class="p">,</span>
    <span class="n">model_executable</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
    <span class="n">kv_caches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">store_status</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">StoreStatus</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Store the KV caches into LMCache for the current model_input.</span>

<span class="sd">    :param model_executable: The model executable for the current request.</span>
<span class="sd">    :type model_executable: torch.nn.Module</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>

<span class="sd">    :param kv_caches: The paged memory to get KV from</span>
<span class="sd">    :type kv_caches: List[torch.Tensor]</span>
<span class="sd">    </span>
<span class="sd">    :param store_status: Indicate whether and how KV cache of each req is stored</span>
<span class="sd">    :type store_status: List[StoreStatus]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">engine</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;LMCache engine is not initialized.&quot;</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">FlashAttentionMetadata</span><span class="p">),</span> \
        <span class="s2">&quot;Only FlashAttention backend is supported for now.&quot;</span>

    <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span>
    <span class="k">assert</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">slot_mapping</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">slot_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">query_start_loc</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">query_start_loc</span>
    <span class="k">assert</span> <span class="n">query_start_loc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">block_tables</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">block_tables</span>

    <span class="c1"># TODO (Jiayi): commenting the following out for now</span>
    <span class="c1"># as Turing architecture is not supported yet</span>
    <span class="c1"># For Turing GPU</span>
    <span class="c1"># num_heads = model_config.get_num_kv_heads(parallel_config)</span>
    <span class="c1"># head_size = model_config.get_head_size()</span>
    <span class="c1"># gpu_capability = torch.cuda.get_device_capability()</span>

    <span class="n">seq_data_idx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>

    <span class="c1"># FIXME(Jiayi): Use `seq_group_list` to determine driver worker</span>
    <span class="c1"># Alternative 1, we can pass in a parameter `is_driver_worker`</span>
    <span class="c1"># Alternative 2, make the broadcast in outside, so the `broadcast`</span>
    <span class="c1"># doesn&#39;t need to be done twice in `lmcache_retrieve` and</span>
    <span class="c1"># `lmcache_store`</span>
    <span class="c1"># We use this dirty fix now as we don&#39;t want to modify the vllm</span>
    <span class="c1"># connector interface for now</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">broadcast_seq_group_list</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">seq_group_list</span>
                                           <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>
    <span class="k">assert</span> <span class="n">seq_group_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">next_start_pos</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">seq_group_idx</span><span class="p">,</span> <span class="n">seq_group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seq_group_list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">seqid</span><span class="p">,</span> <span class="n">seq_data</span> <span class="ow">in</span> <span class="n">seq_group</span><span class="o">.</span><span class="n">seq_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">status</span> <span class="o">=</span> <span class="n">store_status</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">]</span>
            <span class="c1"># TODO (Jiayi): can chunk prefill and vllm prefix</span>
            <span class="c1"># caching use the same logic?</span>
            <span class="k">if</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="n">StoreStatus</span><span class="o">.</span><span class="n">NONE</span><span class="p">]:</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">StoreStatus</span><span class="o">.</span><span class="n">SUFFIX_PREFILL</span><span class="p">,</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span>
            <span class="p">]:</span>
                <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_lens</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_data</span><span class="o">.</span><span class="n">get_len</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="n">StoreStatus</span><span class="o">.</span><span class="n">DECODE</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">seq_len</span> <span class="o">%</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">continue</span>
            <span class="n">current_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">seq_data</span><span class="o">.</span><span class="n">get_token_ids</span><span class="p">()[:</span><span class="n">seq_len</span><span class="p">],</span>
                                          <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

            <span class="n">skip_leading_tokens</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">current_tokens</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">skip_leading_tokens</span> <span class="o">&lt;=</span> <span class="n">seq_len</span>

            <span class="n">vllm_num_required_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">query_start_loc</span><span class="p">[</span><span class="n">seq_data_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span>
                                        <span class="n">query_start_loc</span><span class="p">[</span><span class="n">seq_data_idx</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vllm_num_required_tokens</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>

            <span class="n">start_pos</span> <span class="o">=</span> <span class="n">next_start_pos</span>
            <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">vllm_num_required_tokens</span>
            <span class="n">next_start_pos</span> <span class="o">=</span> <span class="n">end_pos</span>

            <span class="n">vllm_num_computed_tokens</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">-</span> <span class="n">vllm_num_required_tokens</span>
            <span class="k">if</span> <span class="n">vllm_num_computed_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># TODO (Jiayi): what if vllm_num_computed &gt; skip_leading_tokens</span>
                <span class="k">if</span> <span class="n">skip_leading_tokens</span> <span class="o">&gt;=</span> <span class="n">vllm_num_computed_tokens</span><span class="p">:</span>
                    <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">),</span>
                        <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">slot_mapping_req_full</span><span class="p">[</span><span class="n">vllm_num_computed_tokens</span><span class="p">:]</span> <span class="o">=</span> \
                        <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># NOTE(Jiayi): the cache is stored even if it&#39;s in vllm</span>
                    <span class="c1"># as long as it&#39;s not in lmc</span>
                    <span class="k">assert</span> <span class="n">block_tables</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">block_table_full</span> <span class="o">=</span> <span class="n">block_tables</span><span class="p">[</span><span class="n">seq_group_idx</span><span class="p">]</span>
                    <span class="n">vllm_block_size</span> <span class="o">=</span> <span class="n">cache_config</span><span class="o">.</span><span class="n">block_size</span>

                    <span class="n">n_block</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_table_full</span><span class="p">)</span>
                    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
                        <span class="n">vllm_block_size</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_block</span><span class="p">)</span>
                    <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">vllm_block_size</span> \
                        <span class="o">*</span> <span class="n">block_table_full</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">vllm_block_size</span><span class="p">)</span>\
                        <span class="o">+</span> <span class="n">indices</span>
                    <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">slot_mapping_req_full</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">skip_leading_tokens</span> <span class="o">&lt;</span> <span class="n">seq_len</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">skip_leading_tokens</span> <span class="o">%</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span> <span class="o">==</span> <span class="mi">0</span>

                <span class="c1"># TODO(Jiayi): Turing is not supported yet</span>
                <span class="c1"># need to write mem kernels for turing architecture</span>

                <span class="c1"># TODO(Jiayi): prefix caching and chunk prefill</span>
                <span class="c1"># might error here. `slot_mapping_seq` could be wrong</span>

                <span class="n">stored_token_num</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">-</span> <span class="n">skip_leading_tokens</span>
                <span class="n">skipped_token_num</span> <span class="o">=</span> <span class="n">skip_leading_tokens</span>
                <span class="n">kv_tensors_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">current_tokens</span><span class="p">,</span>
                                                  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
                <span class="n">kv_tensors_mask</span><span class="p">[:</span><span class="n">skipped_token_num</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

                <span class="n">engine</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">current_tokens</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                             <span class="n">kv_tensors_mask</span><span class="p">,</span>
                             <span class="n">kvcaches</span><span class="o">=</span><span class="n">kv_caches</span><span class="p">,</span>
                             <span class="n">slot_mapping</span><span class="o">=</span><span class="n">slot_mapping_req_full</span><span class="p">,</span>
                             <span class="n">offset</span><span class="o">=</span><span class="n">skipped_token_num</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">stored_token_num</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">skipped_token_num</span> <span class="o">=</span> <span class="n">seq_len</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Store skips </span><span class="si">{</span><span class="n">skipped_token_num</span><span class="si">}</span><span class="s2"> tokens &quot;</span>\
                    <span class="sa">f</span><span class="s2">&quot;and then stores </span><span class="si">{</span><span class="n">stored_token_num</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>
            <span class="n">seq_data_idx</span> <span class="o">+=</span> <span class="mi">1</span></div>



<div class="viewcode-block" id="lmcache_retrieve_kv">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.lmcache_retrieve_kv">[docs]</a>
<span class="nd">@_lmcache_nvtx_annotate</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lmcache_retrieve_kv</span><span class="p">(</span>
    <span class="n">model_executable</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
    <span class="n">cache_config</span><span class="p">:</span> <span class="n">CacheConfig</span><span class="p">,</span>
    <span class="n">kv_caches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">retrieve_status</span><span class="p">:</span> <span class="n">RetrieveStatus</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Retrieve the KV caches from LMCache for the current model_input. And </span>
<span class="sd">    rebuild the model_input to reflect the changes in KV if necessary.</span>

<span class="sd">    :param model_executable: The model executable for the current request.</span>
<span class="sd">    :type model_executable: torch.nn.Module</span>

<span class="sd">    :param model_input: The model input for the current request.</span>
<span class="sd">    :type model_input: ModelInputForGPUWithSamplingMetadata</span>

<span class="sd">    :param kv_caches: The paged memory to put KV to</span>
<span class="sd">    :type kv_caches: List[torch.Tensor]</span>

<span class="sd">    :param retrieve_status: Indicate whether and how </span>
<span class="sd">                            KV cache of each req is retrieved</span>
<span class="sd">    :type retrieve_status: List[RetrieveStatus]</span>
<span class="sd">    </span>
<span class="sd">    :return: The rebuilt model_input to reflect the changes in KV.</span>
<span class="sd">    :return: The boolean value to indicate whether the </span>
<span class="sd">             entire execute_model should be skipped</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">LMCacheEngineBuilder</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENGINE_NAME</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">engine</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;LMCache engine is not initialized.&quot;</span>

    <span class="k">if</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_blending</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_input</span><span class="p">,</span> <span class="kc">False</span>

    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">FlashAttentionMetadata</span><span class="p">),</span> \
        <span class="s2">&quot;Only FlashAttention backend is supported for now.&quot;</span>

    <span class="n">query_start_loc</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">query_start_loc</span>
    <span class="k">assert</span> <span class="n">query_start_loc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">slot_mapping</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">slot_mapping</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">seq_lens</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span>
    <span class="k">assert</span> <span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># The following metadata are needed to rebuilt the model input</span>
    <span class="n">full_tokens_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_computed_tokens_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lmc_num_computed_tokens_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">start_pos_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">is_prefill_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">next_start_pos</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_request_not_found</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># idx is on a sequence, not a sequence group.</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>

    <span class="c1"># FIXME(Jiayi): Use `seq_group_list` to determine driver worker</span>
    <span class="c1"># Alternative 1, we can pass in a parameter `is_driver_worker`</span>
    <span class="c1"># Alternative 2, make the broadcast in outside, so the `broadcast`</span>
    <span class="c1"># doesn&#39;t need to be done twice in `lmcache_retrieve` and</span>
    <span class="c1"># `lmcache_store`</span>
    <span class="c1"># We use this dirty fix now as we don&#39;t want to modify the vllm</span>
    <span class="c1"># connector interface for now</span>
    <span class="n">model_input</span> <span class="o">=</span> <span class="n">broadcast_seq_group_list</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">seq_group_list</span>
                                           <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">seq_group_list</span> <span class="o">=</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span>
    <span class="k">assert</span> <span class="n">seq_group_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">seq_group</span> <span class="ow">in</span> <span class="n">seq_group_list</span><span class="p">:</span>
        <span class="n">seq_ids</span> <span class="o">=</span> <span class="n">seq_group</span><span class="o">.</span><span class="n">seq_ids</span>
        <span class="k">for</span> <span class="n">seq_id</span> <span class="ow">in</span> <span class="n">seq_ids</span><span class="p">:</span>
            <span class="n">seq_data</span> <span class="o">=</span> <span class="n">seq_group</span><span class="o">.</span><span class="n">seq_data</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span>
            <span class="n">is_prefill_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_group</span><span class="o">.</span><span class="n">is_prompt</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">retrieve_status</span> <span class="o">==</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span><span class="p">:</span>
                <span class="n">total_seq_len</span> <span class="o">=</span> <span class="n">seq_lens</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">total_seq_len</span> <span class="o">=</span> <span class="n">seq_data</span><span class="o">.</span><span class="n">get_len</span><span class="p">()</span>

            <span class="n">full_token_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">seq_data</span><span class="o">.</span><span class="n">get_token_ids</span><span class="p">()[:</span><span class="n">total_seq_len</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="n">full_tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">full_token_tensor</span><span class="p">)</span>

            <span class="n">vllm_num_required_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">query_start_loc</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span>
                                        <span class="n">query_start_loc</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vllm_num_required_tokens</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>

            <span class="n">start_pos</span> <span class="o">=</span> <span class="n">next_start_pos</span>
            <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">vllm_num_required_tokens</span>
            <span class="n">next_start_pos</span> <span class="o">=</span> <span class="n">end_pos</span>
            <span class="n">start_pos_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">start_pos</span><span class="p">)</span>

            <span class="c1"># number of tokens already computed by vllm</span>
            <span class="c1"># (e.g., chunk prefill, prefix caching)</span>
            <span class="n">vllm_num_computed_tokens</span> <span class="o">=</span> <span class="n">total_seq_len</span> <span class="o">-</span> <span class="n">vllm_num_required_tokens</span>

            <span class="c1"># No need to retrieve from lmc if the number of tokens</span>
            <span class="c1"># to be retrieved is small</span>
            <span class="n">lmc_chunk_size</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">chunk_size</span>
            <span class="k">if</span> <span class="n">vllm_num_required_tokens</span> <span class="o">&lt;</span> <span class="n">lmc_chunk_size</span><span class="p">:</span>
                <span class="n">num_computed_tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vllm_num_computed_tokens</span><span class="p">)</span>
                <span class="n">lmc_num_computed_tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">num_request_not_found</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">continue</span>

            <span class="c1"># construct token mesk to indicate what tokens should be retrieved</span>
            <span class="c1"># from lmc. Tokens computed in vllm already should be skipped</span>
            <span class="n">token_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">full_token_tensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
            <span class="n">vllm_num_computed_tokens_align</span> <span class="o">=</span> <span class="n">vllm_num_computed_tokens</span>\
                <span class="o">//</span> <span class="n">lmc_chunk_size</span> <span class="o">*</span> <span class="n">lmc_chunk_size</span>
            <span class="n">token_mask</span><span class="p">[:</span><span class="n">vllm_num_computed_tokens_align</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># TODO(Jiayi): Please get rid of this in the future</span>
            <span class="c1"># Please only pass the required slot_mapping to the engine</span>
            <span class="k">if</span> <span class="n">vllm_num_computed_tokens</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">total_seq_len</span><span class="p">,</span> <span class="p">),</span>
                                                   <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                                                   <span class="n">device</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                                                   <span class="n">dtype</span><span class="o">=</span><span class="n">slot_mapping</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">slot_mapping_req_full</span><span class="p">[</span><span class="n">vllm_num_computed_tokens</span><span class="p">:]</span> <span class="o">=</span> \
                    <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slot_mapping_req_full</span> <span class="o">=</span> <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span>

            <span class="c1"># call lmcache retrieve</span>
            <span class="n">ret_token_mask</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">retrieve</span><span class="p">(</span>
                <span class="n">full_token_tensor</span><span class="p">,</span>
                <span class="n">token_mask</span><span class="p">,</span>
                <span class="n">kvcaches</span><span class="o">=</span><span class="n">kv_caches</span><span class="p">,</span>
                <span class="n">slot_mapping</span><span class="o">=</span><span class="n">slot_mapping_req_full</span><span class="p">)</span>
            <span class="n">lmc_num_computed_tokens</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ret_token_mask</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> \
                    <span class="p">(</span><span class="n">vllm_num_computed_tokens</span> <span class="o">-</span> <span class="n">vllm_num_computed_tokens_align</span><span class="p">),</span>
                    <span class="mi">0</span>
                <span class="p">)</span>

            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lmc_num_computed_tokens</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>

            <span class="c1"># total number of computed tokens (vllm + lmc)</span>
            <span class="n">num_computed_tokens</span> <span class="o">=</span> <span class="n">vllm_num_computed_tokens</span> <span class="o">+</span> \
                <span class="n">lmc_num_computed_tokens</span>

            <span class="c1"># TODO(Jiayi): currently we do not skip anything if chunked prefill</span>
            <span class="c1"># is batched with any decode or other chunked prefills.</span>
            <span class="k">if</span> <span class="n">retrieve_status</span> <span class="o">==</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">num_computed_tokens</span> <span class="o">!=</span> <span class="n">total_seq_len</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">model_input</span><span class="p">,</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Avoid error when prefix is exactly the same as the retrieved</span>
                <span class="c1"># However, the entire prefill should be skipped in chunk prefill</span>
                <span class="k">if</span> <span class="n">num_computed_tokens</span> <span class="o">==</span> <span class="n">total_seq_len</span><span class="p">:</span>
                    <span class="n">lmc_num_computed_tokens</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="n">num_computed_tokens</span> <span class="o">-=</span> <span class="mi">1</span>

            <span class="n">num_computed_tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_computed_tokens</span><span class="p">)</span>
            <span class="n">lmc_num_computed_tokens_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lmc_num_computed_tokens</span><span class="p">)</span>

            <span class="c1"># No cache found, move on</span>
            <span class="k">if</span> <span class="n">lmc_num_computed_tokens</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">num_request_not_found</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Inject the lmc retrieved kv cache</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Injected token number: </span><span class="si">{</span><span class="n">lmc_num_computed_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">seq_cnt</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">query_start_loc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">seq_cnt</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">lmc_num_computed_tokens_list</span><span class="p">)</span> <span class="o">==</span> <span class="n">seq_cnt</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_computed_tokens_list</span><span class="p">)</span> <span class="o">==</span> <span class="n">seq_cnt</span>

    <span class="k">if</span> <span class="n">retrieve_status</span> <span class="o">==</span> <span class="n">RetrieveStatus</span><span class="o">.</span><span class="n">CHUNK_PREFILL</span> <span class="ow">and</span> \
        <span class="n">num_request_not_found</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_input</span><span class="p">,</span> <span class="kc">True</span>

    <span class="c1"># Some of the request can be skipped for a bit</span>
    <span class="c1"># TODO(Jiayi): need e2e test full prefill and partial prefill</span>
    <span class="c1"># in a single batch</span>
    <span class="k">if</span> <span class="n">num_request_not_found</span> <span class="o">&lt;</span> <span class="n">seq_cnt</span><span class="p">:</span>
        <span class="n">rebuilt_model_input</span> <span class="o">=</span> <span class="n">build_partial_prefill_input</span><span class="p">(</span>
            <span class="n">model_input</span><span class="p">,</span>
            <span class="n">full_tokens_list</span><span class="p">,</span>
            <span class="n">num_computed_tokens_list</span><span class="p">,</span>
            <span class="n">start_pos_list</span><span class="p">,</span>
            <span class="n">slot_mapping</span><span class="p">,</span>
            <span class="n">lmc_num_computed_tokens_list</span><span class="p">,</span>
            <span class="n">is_prefill_list</span><span class="p">,</span>
            <span class="n">kv_caches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">cache_config</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Rebuilt the input!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rebuilt_model_input</span><span class="p">,</span> <span class="kc">False</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Returning the original input!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_input</span><span class="p">,</span> <span class="kc">False</span></div>



<div class="viewcode-block" id="build_partial_prefill_input">
<a class="viewcode-back" href="../../../../advanced/lmcache.integration.vllm.html#lmcache.integration.vllm.vllm_adapter.build_partial_prefill_input">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">build_partial_prefill_input</span><span class="p">(</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">,</span>
    <span class="n">full_tokens_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">num_computed_tokens_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">start_pos_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">slot_mapping_flat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">lmc_num_computed_tokens_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">is_prefill_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">cache_config</span><span class="p">:</span> <span class="n">CacheConfig</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ModelInputForGPUWithSamplingMetadata&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper function to rebuild the model input for the current request.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">FlashAttentionMetadata</span><span class="p">),</span> \
        <span class="s2">&quot;Only FlashAttention backend is supported for now.&quot;</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">context_lens_tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">block_tables</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">query_start_loc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="n">model_input</span><span class="o">.</span><span class="n">input_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">rebuilt_input_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rebuilt_input_positions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rebuilt_query_lens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rebuilt_num_prefills</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">rebuilt_num_prefill_tokens</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">rebuilt_slot_mapping</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rebuilt_max_query_len</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">rebuilt_block_tables</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">rebuilt_query_start_loc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">rebuilt_context_lens_tensor</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rebuilt_selected_token_indices</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">last_query_start_loc</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># recounting query and context lengths</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">full_tokens_list</span><span class="p">)):</span>
        <span class="n">token_tensor</span> <span class="o">=</span> <span class="n">full_tokens_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">num_token</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_tensor</span><span class="p">)</span>
        <span class="n">num_computed_token</span> <span class="o">=</span> <span class="n">num_computed_tokens_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">start_pos</span> <span class="o">=</span> <span class="n">start_pos_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">is_prefill</span> <span class="o">=</span> <span class="n">is_prefill_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">lmc_num_computed_tokens</span> <span class="o">=</span> <span class="n">lmc_num_computed_tokens_list</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">rebuilt_input_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_tensor</span><span class="p">[</span><span class="n">num_computed_token</span><span class="p">:])</span>
        <span class="n">q_len</span> <span class="o">=</span> <span class="n">num_token</span> <span class="o">-</span> <span class="n">num_computed_token</span>
        <span class="k">assert</span> <span class="n">q_len</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">rebuilt_query_lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q_len</span><span class="p">)</span>
        <span class="n">start_input_pos_idx</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">lmc_num_computed_tokens</span>
        <span class="n">end_input_pos_idx</span> <span class="o">=</span> <span class="n">start_input_pos_idx</span> <span class="o">+</span> <span class="n">q_len</span>
        <span class="n">rebuilt_input_positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">model_input</span><span class="o">.</span><span class="n">input_positions</span><span class="p">[</span><span class="n">start_input_pos_idx</span><span class="p">:</span><span class="n">end_input_pos_idx</span><span class="p">])</span>
        <span class="c1"># Attn metadata-related</span>
        <span class="k">if</span> <span class="n">is_prefill</span><span class="p">:</span>
            <span class="n">rebuilt_num_prefills</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">rebuilt_num_prefill_tokens</span> <span class="o">+=</span> <span class="n">q_len</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">q_len</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="n">start_slot_idx</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">lmc_num_computed_tokens</span>
        <span class="n">end_slot_idx</span> <span class="o">=</span> <span class="n">start_slot_idx</span> <span class="o">+</span> <span class="n">q_len</span>
        <span class="n">new_slot_mapping</span> <span class="o">=</span> <span class="n">slot_mapping_flat</span><span class="p">[</span><span class="n">start_slot_idx</span><span class="p">:</span><span class="n">end_slot_idx</span><span class="p">]</span>
        <span class="n">rebuilt_slot_mapping</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_slot_mapping</span><span class="p">)</span>
        <span class="n">rebuilt_max_query_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_len</span><span class="p">,</span> <span class="n">rebuilt_max_query_len</span><span class="p">)</span>

        <span class="n">last_query_start_loc</span> <span class="o">+=</span> <span class="n">q_len</span>
        <span class="n">rebuilt_query_start_loc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_query_start_loc</span><span class="p">)</span>  <span class="c1"># start with 0</span>
        <span class="n">rebuilt_context_lens_tensor</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">num_computed_token</span><span class="p">)</span>

        <span class="c1"># recover `block_table`</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">rebuilt_block_tables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">block_tables</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slot_mapping_req</span> <span class="o">=</span> <span class="n">slot_mapping_flat</span><span class="p">[</span><span class="n">start_pos</span><span class="p">:</span><span class="n">end_slot_idx</span><span class="p">]</span>
            <span class="n">vllm_block_size</span> <span class="o">=</span> <span class="n">cache_config</span><span class="o">.</span><span class="n">block_size</span>
            <span class="n">rebuilt_block_table</span> <span class="o">=</span> <span class="n">slot_mapping_req</span><span class="p">[::</span><span class="mi">16</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> \
                <span class="o">//</span> <span class="n">vllm_block_size</span>
            <span class="n">rebuilt_block_tables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rebuilt_block_table</span><span class="p">)</span>

        <span class="c1"># Sampling metadata related</span>
        <span class="c1"># seq_groups (use rebuilt query lens)</span>
        <span class="n">rebuilt_selected_token_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_query_start_loc</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># rebuilt attn_metadata</span>
    <span class="n">rebuilt_attn_metadata</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="p">)</span>
    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">num_prefills</span> <span class="o">=</span> <span class="n">rebuilt_num_prefills</span>
    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">num_prefill_tokens</span> <span class="o">=</span> <span class="n">rebuilt_num_prefill_tokens</span>
    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">rebuilt_slot_mapping</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">device</span><span class="p">)</span>
    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">max_query_len</span> <span class="o">=</span> <span class="n">rebuilt_max_query_len</span>

    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">block_tables</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">rebuilt_block_tables</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">query_start_loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">rebuilt_query_start_loc</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">query_start_loc</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">context_lens_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="n">rebuilt_context_lens_tensor</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">context_lens_tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">rebuilt_attn_metadata</span><span class="o">.</span><span class="n">_cached_prefill_metadata</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">rebuilt_sampling_metadata</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># rebuilt sampling_metadata</span>
    <span class="k">if</span> <span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rebuilt_sampling_metadata</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">q_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rebuilt_query_lens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">rebuilt_sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">rebuilt_sampling_metadata</span><span class="o">.</span><span class="n">seq_groups</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">query_len</span> <span class="o">=</span> <span class="n">q_len</span>

        <span class="n">rebuilt_sampling_metadata</span><span class="o">.</span><span class="n">selected_token_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">rebuilt_selected_token_indices</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">sampling_metadata</span><span class="o">.</span><span class="n">selected_token_indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># import here to avoid circular import.</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.worker.model_runner</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelInputForGPUWithSamplingMetadata</span>
    <span class="n">rebuilt_model_input</span> <span class="o">=</span> <span class="n">ModelInputForGPUWithSamplingMetadata</span><span class="p">(</span>
        <span class="n">input_tokens</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">rebuilt_input_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="n">input_positions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">rebuilt_input_positions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="n">seq_lens</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">,</span>
        <span class="n">query_lens</span><span class="o">=</span><span class="n">rebuilt_query_lens</span><span class="p">,</span>
        <span class="n">lora_mapping</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">lora_mapping</span><span class="p">,</span>
        <span class="n">lora_requests</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">lora_requests</span><span class="p">,</span>
        <span class="n">attn_metadata</span><span class="o">=</span><span class="n">rebuilt_attn_metadata</span><span class="p">,</span>
        <span class="n">prompt_adapter_mapping</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">prompt_adapter_mapping</span><span class="p">,</span>
        <span class="n">prompt_adapter_requests</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">prompt_adapter_requests</span><span class="p">,</span>
        <span class="n">multi_modal_kwargs</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">multi_modal_kwargs</span><span class="p">,</span>
        <span class="n">request_ids_to_seq_ids</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">request_ids_to_seq_ids</span><span class="p">,</span>
        <span class="n">finished_requests_ids</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">finished_requests_ids</span><span class="p">,</span>
        <span class="n">virtual_engine</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">virtual_engine</span><span class="p">,</span>
        <span class="n">sampling_metadata</span><span class="o">=</span><span class="n">rebuilt_sampling_metadata</span><span class="p">,</span>
        <span class="n">is_prompt</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">is_prompt</span><span class="p">,</span>
        <span class="n">async_callback</span><span class="o">=</span><span class="n">model_input</span><span class="o">.</span><span class="n">async_callback</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">rebuilt_model_input</span></div>

</pre></div>
    </div><div class="flex justify-between items-center pt-6 mt-12 border-t border-border gap-4">
</div></div>
        </main>
      </div>
    </div><footer class="py-6 border-t border-border md:py-0">
    <div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
      <div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
        <p class="text-sm leading-loose text-center text-muted-foreground md:text-left">Â© 2024, The LMCache Team&nbsp;Built with <a class="font-medium underline underline-offset-4"
    href="https://www.sphinx-doc.org"
    rel="noreferrer">Sphinx 7.4.7</a></p>
</div>
</div>
</footer>
  </div>
  
    <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script defer="defer" src="../../../../_static/theme.js?v=ab60c18a"></script>
  
</body>
</html>
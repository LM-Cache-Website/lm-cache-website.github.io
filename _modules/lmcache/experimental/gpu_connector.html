<!DOCTYPE html>
<html lang="en"
      data-content_root="../../../"
      x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }"
      x-init="$watch('darkMode', val => localStorage.setItem('darkMode', val))"
      class="scroll-smooth"
      :class="{'dark': darkMode === 'dark' || (darkMode === 'system' && window.matchMedia('(prefers-color-scheme: dark)').matches)}"
>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta charset="utf-8" />
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="white" />
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="black" />
  
    <title>lmcache.experimental.gpu_connector | LMCache</title>
    <meta property="og:title" content="lmcache.experimental.gpu_connector | LMCache" />
    <meta name="twitter:title" content="lmcache.experimental.gpu_connector | LMCache" />
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../_static/theme.css?v=caad1007" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="icon" href="../../../_static/lmcache-logo.png" />
        <link rel="search" title="Search" href="../../../search.html" />
        <link rel="index" title="Index" href="../../../genindex.html" />

    <script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
</head>
<body x-data="{ showSidebar: false }" class="min-h-screen font-sans antialiased bg-background text-foreground" :class="{ 'overflow-hidden': showSidebar }">
    <div x-cloak x-show="showSidebar" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm md:hidden" @click.self="showSidebar = false"></div><div id="page" class="relative flex flex-col min-h-screen"><a href="#content" class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100">
      Skip to content
    </a><header
  class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
    <div class="hidden mr-4 md:flex">
      <a href="../../../index.html" class="flex items-center mr-6"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">LMCache</span>
      </a></div><button
      class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden"
      type="button" @click="showSidebar = true">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" aria-hidden="true"
        fill="currentColor">
        <path
          d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z" />
      </svg>
      <span class="sr-only">Toggle navigation menu</span>
    </button>
    <div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
      <div class="flex-1 w-full md:w-auto md:flex-none"><form id="searchbox"
      action="../../../search.html"
      method="get"
      class="relative flex items-center group"
      @keydown.k.window.meta="$refs.search.focus()">
  <input x-ref="search"
          name="q"
          id="search-input"
          type="search"
          aria-label="Search the docs"
          placeholder="Search ..."
          class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" />
  <kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
    <span class="text-xs">âŒ˜</span>
    K
  </kbd>
</form>
      </div>
      <nav class="flex items-center space-x-1">
        <a href="https://github.com/LMCache/LMCache/" title="Visit GitHub" rel="noopener nofollow">
          <div
            class="inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md disabled:opacity-50 disabled:pointer-events-none hover:bg-accent hover:text-accent-foreground h-9 w-9">
            <svg height="26px" style="margin-top:-2px;display:inline" viewBox="0 0 45 44" fill="currentColor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M22.477.927C10.485.927.76 10.65.76 22.647c0 9.596 6.223                 17.736 14.853 20.608 1.087.2 1.483-.47 1.483-1.047 0-.516-.019-1.881-.03-3.693-6.04 1.312-7.315-2.912-7.315-2.912-.988-2.51-2.412-3.178-2.412                 -3.178-1.972-1.346.149-1.32.149-1.32 2.18.154 3.327 2.24 3.327 2.24 1.937 3.318 5.084 2.36 6.321 1.803.197-1.403.759-2.36 1.379-2.903-4.823-.548-9.894-2.412-9.894-10.734 0-2.37.847-4.31 2.236-5.828-.224-.55-.969-2.759.214-5.748 0 0 1.822-.584 5.972 2.226 1.732-.482 3.59-.722 5.437-.732 1.845.01 3.703.25 5.437.732 4.147-2.81 5.967-2.226 5.967-2.226 1.185 2.99.44 5.198.217 5.748 1.392 1.517 2.232                  3.457 2.232 5.828 0 8.344-5.078 10.18-9.916 10.717.779.67 1.474 1.996 1.474                 4.021 0 2.904-.027 5.247-.027 5.96 0 .58.392 1.256 1.493 1.044C37.981 40.375 44.2 32.24                  44.2 22.647c0-11.996-9.726-21.72-21.722-21.72" fill="currentColor"/></svg>
          </div>
        </a>
        
        <button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'"
          class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9"
          type="button"
          aria-label="Color theme switcher">
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0">
            <path
              d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z" />
          </svg>
          <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
            class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100">
            <path
              d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z" />
          </svg>
        </button>
      </nav>
    </div>
  </div>
</header>

    <div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside id="left-sidebar"
  class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky"
  :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }">

    <a href="../../../index.html" class="!justify-start text-sm md:!hidden bg-background"><span class="font-bold text-clip whitespace-nowrap">LMCache</span>
    </a>

    <div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
      <div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
  <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/speedup.html">3 Min Quick Example: Docker + Speedup with vLLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/docker.html">Docker Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/quickstart.html">Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Configure LMCache</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../configuration/v1/index.html">LMCache v1</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../configuration/v1/v1_config.html">Configuring LMCache v1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../configuration/v0/index.html">LMCache v0</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../configuration/v0/v0_config.html">Configuring LMCache v0</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Detailed Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/v1/index.html">LMCache v1</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v1/cpu_offload.html">CPU Offloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v1/disagg.html">Disaggregated prefill</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v1/p2p.html">P2P KV Cache Sharing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/v0/index.html">LMCache v0</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v0/backend.html">Selecting a backend</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v0/launching.html">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v0/save_decode.html">Saving the decode cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/v0/kv_blending.html">KV blending</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../models/models.html">Supported Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/overview.html">LMCache Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/EngineInterface.html">LMCache Engine Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/GPUConnectorInterface.html">LMCache GPU Connector Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/MemoryObject.html">LMCache Memory Object</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/MemoryInterface.html">LMCache Memory Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer_tutorial/BackendInterface.html">LMCache Backend Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/KVBlend.html">KV Blending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/KVEvictor.html">KVCache Evictor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../advanced/vLLMIntegration.html">vLLM Integration</a></li>
</ul>

</nav>
      </div>
    </div>
    <button type="button" @click="showSidebar = false"
      class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100">
      <svg xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 96 960 960" fill="currentColor"
        stroke="none" class="h-4 w-4">
        <path
          d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z" />
      </svg>
    </button>
  </aside>
        <main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs"
     class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
  <a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground"
     href="../../../index.html">
    <span class="hidden md:inline">LMCache</span>
    <svg xmlns="http://www.w3.org/2000/svg"
         height="18"
         width="18"
         viewBox="0 96 960 960"
         aria-label="Home"
         fill="currentColor"
         stroke="none"
         class="md:hidden">
      <path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z" />
    </svg>
  </a>
  
<div class="mr-1">/</div><a class="hover:text-foreground overflow-hidden text-ellipsis whitespace-nowrap"
       href="../../index.html">Module code</a>
    
<div class="mr-1">/</div><span aria-current="page"
        class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">lmcache.experimental.gpu_connector</span>
</nav>

    <div id="content" role="main">
      <h1>Source code for lmcache.experimental.gpu_connector</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">abc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">lmcache.c_ops</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lmc_ops</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.experimental.memory_management</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryFormat</span><span class="p">,</span> <span class="n">MemoryObj</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lmcache.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_lmcache_nvtx_annotate</span>


<div class="viewcode-block" id="GPUConnectorInterface">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.GPUConnectorInterface">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GPUConnectorInterface</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">):</span>

<div class="viewcode-block" id="GPUConnectorInterface.to_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.GPUConnectorInterface.to_gpu">[docs]</a>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># FIXME (Yihua): We shouldn&#39;t put start and end here since</span>
        <span class="c1"># it&#39;s not the responsibility of the GPUConnector to know</span>
        <span class="c1"># the token-sequence-related information.</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store the data in the memory object into a GPU buffer.</span>
<span class="sd">        Sub-classes should define the format of the kwargs.</span>

<span class="sd">        :param MemoryObj memory_obj: The memory object to be copied into GPU.</span>
<span class="sd">        :param int start: The starting index of the data in the corresponding</span>
<span class="sd">            token sequence.</span>
<span class="sd">        :param int end: The ending index of the data in the corresponding</span>
<span class="sd">            token sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="GPUConnectorInterface.from_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.GPUConnectorInterface.from_gpu">[docs]</a>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># FIXME (Yihua): We shouldn&#39;t put start and end here since</span>
        <span class="c1"># it&#39;s not the responsibility of the GPUConnector to know</span>
        <span class="c1"># the token-sequence-related information.</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the data from a GPU buffer into the memory object.</span>
<span class="sd">        Sub-classes should define the format of the kwargs.</span>

<span class="sd">        :param MemoryObj memory_obj: The memory object to store the data from </span>
<span class="sd">            GPU.</span>
<span class="sd">        :param int start: The starting index of the data in the corresponding</span>
<span class="sd">            token sequence.</span>
<span class="sd">        :param int end: The ending index of the data in the corresponding</span>
<span class="sd">            token sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="GPUConnectorInterface.get_shape">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.GPUConnectorInterface.get_shape">[docs]</a>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the shape of the data given the number of tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
</div>



<div class="viewcode-block" id="VLLMNestedTupleGPUConnector">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMNestedTupleGPUConnector">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMNestedTupleGPUConnector</span><span class="p">(</span><span class="n">GPUConnectorInterface</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The GPU KV cache should be a nested tuple of K and V tensors.</span>
<span class="sd">    More specifically, we have:</span>
<span class="sd">    - GPUTensor = Tuple[KVLayer, ...]</span>
<span class="sd">    - KVLayer = Tuple[Tensor, Tensor]</span>
<span class="sd">    - Tensor: [num_tokens, ...]</span>

<span class="sd">    The token dimension is specified by `token_dim` when constructing the</span>
<span class="sd">    connector.</span>

<span class="sd">    It will produce / consume memory object with KV_BLOB format</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param int gpu_token_dim: The token dimension of the GPU KV cache in</span>
<span class="sd">            the nested tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span> <span class="o">=</span> <span class="n">hidden_dim_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

    <span class="c1"># TODO(Jiayi): fix the gpu memory</span>
<div class="viewcode-block" id="VLLMNestedTupleGPUConnector.to_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMNestedTupleGPUConnector.to_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs.</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">!=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The memory object should be in KV_BLOB format in&quot;</span>
                <span class="s2">&quot; order to be processed by NestedTupleGPUConnector&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">):</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">layer</span>
            <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">k</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">hidden_shape</span><span class="p">),</span>
                               <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">v</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">hidden_shape</span><span class="p">),</span>
                               <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="VLLMNestedTupleGPUConnector.from_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMNestedTupleGPUConnector.from_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs, or the </span>
<span class="sd">            memory object is not in KV_BLOB format.</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>

        <span class="n">put_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
        <span class="c1"># Wait for all operations on the default stream to finish</span>
        <span class="n">put_stream</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">(</span>
            <span class="n">kvcaches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">):</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">layer</span>
            <span class="n">k</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">put_stream</span><span class="p">)</span>
            <span class="n">v</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">put_stream</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">put_stream</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">):</span>
                <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">layer</span>
                <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
                                                     <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">k</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
                                                     <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">put_stream</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span></div>


<div class="viewcode-block" id="VLLMNestedTupleGPUConnector.get_shape">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMNestedTupleGPUConnector.get_shape">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span><span class="p">])</span></div>
</div>



<div class="viewcode-block" id="VLLMPagedMemGPUConnector">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnector">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMPagedMemGPUConnector</span><span class="p">(</span><span class="n">GPUConnectorInterface</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The GPU KV cache should be a nested tuple of K and V tensors.</span>
<span class="sd">    More specifically, we have:</span>
<span class="sd">    - GPUTensor = Tuple[KVLayer, ...]</span>
<span class="sd">    - KVLayer = Tuple[Tensor, Tensor]</span>
<span class="sd">    - Tensor: [num_blocks, block_size, num_heads, head_size]</span>

<span class="sd">    It will produce / consume memory object with KV_BLOB format</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param int gpu_token_dim: The token dimension of the GPU KV cache in</span>
<span class="sd">            the nested tuple.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span> <span class="o">=</span> <span class="n">hidden_dim_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>

<div class="viewcode-block" id="VLLMPagedMemGPUConnector.to_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnector.to_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs.</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        :raises ValueError: If &#39;slot_mapping&#39; is not provided in kwargs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">!=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The memory object should be in KV_BLOB format in&quot;</span>
                <span class="s2">&quot; order to be processed by VLLMPagedMemGPUConnector&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;slot_mapping&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;slot_mapping&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>
        <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;slot_mapping&quot;</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">):</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">lmc_ops</span><span class="o">.</span><span class="n">reshape_and_cache_back_flash</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
                                                 <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
                                                 <span class="n">layer_id</span><span class="p">)</span></div>


        <span class="c1"># TODO(Jiayi): Currently, this is a blocking operation.</span>
        <span class="c1"># We might be able to continue other decode jobs while</span>
        <span class="c1"># waiting for the copy to finish.</span>

<div class="viewcode-block" id="VLLMPagedMemGPUConnector.from_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnector.from_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs, or the </span>
<span class="sd">            memory object is not in KV_BLOB format.</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        :raises ValueError: If &#39;slot_mapping&#39; is not provided in kwargs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;slot_mapping&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;slot_mapping&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;offset&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">start</span> <span class="o">-</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">]</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">start</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">end</span> <span class="o">&gt;=</span> <span class="n">start</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>
        <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;slot_mapping&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">):</span>
            <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">lmc_ops</span><span class="o">.</span><span class="n">load_and_reshape_flash</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span>
                                           <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">layer_id</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span></div>


<div class="viewcode-block" id="VLLMPagedMemGPUConnector.get_shape">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnector.get_shape">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span><span class="p">])</span></div>
</div>



<div class="viewcode-block" id="VLLMPagedMemGPUConnectorV2">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnectorV2">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMPagedMemGPUConnectorV2</span><span class="p">(</span><span class="n">GPUConnectorInterface</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The GPU KV cache should be a nested tuple of K and V tensors.</span>
<span class="sd">    More specifically, we have:</span>
<span class="sd">    - GPUTensor = Tuple[KVLayer, ...]</span>
<span class="sd">    - KVLayer = Tuple[Tensor, Tensor]</span>
<span class="sd">    - Tensor: [num_blocks, block_size, num_heads, head_size]</span>

<span class="sd">    It will produce / consume memory object with KV_BLOB format</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_dim_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">use_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If use_gpu is true, it will create a gpu intermediate buffer. In this </span>
<span class="sd">        case, it requires the following kwargs:</span>
<span class="sd">        - chunk_size: The MAX size of the chunk to be copied to GPU.</span>
<span class="sd">        - dtype: The data type of the intermediate buffer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span> <span class="o">=</span> <span class="n">hidden_dim_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_pointers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                                             <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
                                             <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointers_initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">page_buffer_size</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
            <span class="k">assert</span> <span class="s2">&quot;chunk_size&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">,</span> \
                    <span class="s2">&quot;chunk_size should be provided to create a GPU buffer.&quot;</span>
            <span class="k">assert</span> <span class="s2">&quot;dtype&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">,</span> \
                    <span class="s2">&quot;dtype should be provided to create a GPU buffer.&quot;</span>
            <span class="k">assert</span> <span class="s2">&quot;device&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">,</span> \
                    <span class="s2">&quot;device should be provided to create a GPU buffer.&quot;</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;chunk_size&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">],</span>
                                          <span class="n">device</span><span class="o">=</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_pointers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kv_caches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_pointers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kv_caches</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pointers_initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># kv_caches[0].shape: [2, num_pages, page_size, num_heads, head_size]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">page_buffer_size</span> <span class="o">=</span> <span class="n">kv_caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">kv_caches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<div class="viewcode-block" id="VLLMPagedMemGPUConnectorV2.to_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnectorV2.to_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        Note: </span>
<span class="sd">          1. This function expects the &#39;slot_mapping&#39; is a &quot;full slot mapping&quot;</span>
<span class="sd">             where it&#39;s length is the same as the whole token sequence.</span>
<span class="sd">          2. In the case that there is prefix caching, slot_mapping will starts</span>
<span class="sd">             with -1s until the end of the matched prefix. The start and end</span>
<span class="sd">             should NEVER overlap with the prefix caching (which means the </span>
<span class="sd">             underlying CUDA kernel will never see -1 in slot_mapping)</span>


<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs.</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        :raises ValueError: If &#39;slot_mapping&#39; is not provided in kwargs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">!=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The memory object should be in KV_BLOB format in&quot;</span>
                <span class="s2">&quot; order to be processed by VLLMPagedMemGPUConnector&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;slot_mapping&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;slot_mapping&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>
        <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;slot_mapping&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointers_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_pointers</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">)</span>

        <span class="c1"># NOTE(ApostaC): By default, detour from a GPU buffer is slower</span>
        <span class="c1"># than directly copying from the CPU.</span>
        <span class="c1"># So disabling it for now and use direct copy from CPU to GPU.</span>

        <span class="c1">#if self.gpu_buffer is None or \</span>
        <span class="c1">#        end - start != self.gpu_buffer.shape[2]:</span>
        <span class="c1">#    lmc_ops.multi_layer_kv_transfer(memory_obj.tensor,</span>
        <span class="c1">#                                    self.kv_cache_pointers,</span>
        <span class="c1">#                                    slot_mapping[start:end],</span>
        <span class="c1">#                                    kvcaches[0].device,</span>
        <span class="c1">#                                    self.page_buffer_size, False)</span>
        <span class="c1">#else:</span>
        <span class="c1">#    # Memobj -&gt; gpu_buffer -&gt; kvcaches</span>
        <span class="c1">#    assert self.gpu_buffer.device == kvcaches[0].device</span>
        <span class="c1">#    tmp_gpu_buffer = self.gpu_buffer[:, :, :end-start, :]</span>
        <span class="c1">#    tmp_gpu_buffer.copy_(memory_obj.tensor, non_blocking=True)</span>
        <span class="c1">#    lmc_ops.multi_layer_kv_transfer(</span>
        <span class="c1">#        tmp_gpu_buffer,</span>
        <span class="c1">#        self.kv_cache_pointers,</span>
        <span class="c1">#        slot_mapping[start:end],</span>
        <span class="c1">#        kvcaches[0].device, self.page_buffer_size, False)</span>

        <span class="n">lmc_ops</span><span class="o">.</span><span class="n">multi_layer_kv_transfer</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_pointers</span><span class="p">,</span>
                                        <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
                                        <span class="n">kvcaches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">page_buffer_size</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span></div>


<div class="viewcode-block" id="VLLMPagedMemGPUConnectorV2.from_gpu">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnectorV2.from_gpu">[docs]</a>
    <span class="nd">@_lmcache_nvtx_annotate</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory_obj</span><span class="p">:</span> <span class="n">MemoryObj</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Expect a kwarg &#39;kvcaches&#39; which is a nested tuple of K and V tensors.</span>
<span class="sd">        The kvcaches should correspond to the &quot;WHOLE token sequence&quot;.</span>

<span class="sd">        Will set the memory_obj.metadata.fmt to MemoryFormat.KV_BLOB.</span>

<span class="sd">        Note: </span>
<span class="sd">          1. This function expects the &#39;slot_mapping&#39; is a &quot;full slot mapping&quot;</span>
<span class="sd">             where it&#39;s length is the same as the whole token sequence.</span>
<span class="sd">          2. In the case that there is prefix caching, slot_mapping will starts</span>
<span class="sd">             with -1s until the end of the matched prefix. The start and end</span>
<span class="sd">             should NEVER overlap with the prefix caching (which means the </span>
<span class="sd">             underlying CUDA kernel will never see -1 in slot_mapping)</span>

<span class="sd">        :raises ValueError: If &#39;kvcaches&#39; is not provided in kwargs,</span>
<span class="sd">        :raises AssertionError: If the memory object does not have a tensor.</span>
<span class="sd">        :raises ValueError: If &#39;slot_mapping&#39; is not provided in kwargs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="s2">&quot;kvcaches&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;kvcaches&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;slot_mapping&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;slot_mapping&#39; should be provided in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">kvcaches</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;kvcaches&quot;</span><span class="p">]</span>
        <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;slot_mapping&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointers_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_pointers</span><span class="p">(</span><span class="n">kvcaches</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> \
                <span class="n">end</span> <span class="o">-</span> <span class="n">start</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">lmc_ops</span><span class="o">.</span><span class="n">multi_layer_kv_transfer</span><span class="p">(</span><span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_pointers</span><span class="p">,</span>
                                            <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
                                            <span class="n">kvcaches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">page_buffer_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># kvcaches -&gt; gpu_buffer -&gt; memobj</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">kvcaches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
            <span class="n">tmp_gpu_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_buffer</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">lmc_ops</span><span class="o">.</span><span class="n">multi_layer_kv_transfer</span><span class="p">(</span><span class="n">tmp_gpu_buffer</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_pointers</span><span class="p">,</span>
                                            <span class="n">slot_mapping</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
                                            <span class="n">kvcaches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">page_buffer_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">memory_obj</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tmp_gpu_buffer</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">memory_obj</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">fmt</span> <span class="o">=</span> <span class="n">MemoryFormat</span><span class="o">.</span><span class="n">KV_BLOB</span></div>


<div class="viewcode-block" id="VLLMPagedMemGPUConnectorV2.get_shape">
<a class="viewcode-back" href="../../../developer_tutorial/GPUConnectorInterface.html#lmcache.experimental.gpu_connector.VLLMPagedMemGPUConnectorV2.get_shape">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
            <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim_size</span><span class="p">])</span></div>
</div>

</pre></div>
    </div><div class="flex justify-between items-center pt-6 mt-12 border-t border-border gap-4">
</div></div>
        </main>
      </div>
    </div><footer class="py-6 border-t border-border md:py-0">
    <div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
      <div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
        <p class="text-sm leading-loose text-center text-muted-foreground md:text-left">Â© 2024, The LMCache Team&nbsp;Built with <a class="font-medium underline underline-offset-4"
    href="https://www.sphinx-doc.org"
    rel="noreferrer">Sphinx 7.4.7</a></p>
</div>
</div>
</footer>
  </div>
  
    <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=34893b04"></script>
    <script defer="defer" src="../../../_static/theme.js?v=ab60c18a"></script>
  
</body>
</html>